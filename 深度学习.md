[toc]

# 线性神经网络
## 1.线性模型
### **基本元素**
为了解释线性回归，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为训练数据集（training data set） 或训练集（training set）。 每行数据（比如一次房屋交易相对应的数据）称为样本（sample）， 也可以称为数据点（data point）或数据样本（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。 预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。

### **模型**
设特征为$x\in \mathbb{R}^{d}$，权重为$w\in \mathbb{R}^{d}$，则模型可表示为
$$\hat{y}=w^{T}x+b$$

``` python
def linreg(X, w, b):  #@save
    """线性回归模型"""
    return torch.matmul(X, w) + b
```

或者是
``` python
# nn是神经网络的缩写
from torch import nn

net = nn.Sequential(nn.Linear(2, 1))
```

在开始寻找最好的模型参数（model parameters）$w,b$之前，我们还需要两样东西：
1. 模型质量的度量方式
2. 能够更新模型以提高模型预测质量的方法

### **损失函数**
损失函数（loss function）能够量化目标的实际值与预测值之间的差距。通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。回归问题中最常用的损失函数是平方误差函数。当样本$i$的预测值为$\hat{y}^{(i)}$，而其标签真实值为$y^{(i)}$，平方误差可以定义为：
$$l^{(i)}(w,b)=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^{2}$$

``` python
def squared_loss(y_hat, y):  #@save
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

或者是

``` python
loss = nn.MSELoss()
```

为了进一步说明，来看下面的例子。 我们为一维情况下的回归问题绘制图像，如下图

<div align=center><img src=../机器学习入门/深度学习/fit-linreg.svg></div>

由于平方误差函数中的二次方项，估计值$\hat{y}^{(i)}$和观测值$\hat{y}^{(i)}$之间较大的差异将导致更大的损失。为了度量模型在整个数据集上的质量，我们需计算在训练集$n$个样本上的损失均值（也等价于求和）
$$L(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^{n} l^{(i)}(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right)^{2}$$
在训练模型时，我们希望寻找一组参数$（w^{*},b^{*}）$， 这组参数能最小化在所有训练样本上的总损失
$$w^{*},b^{*}=\arg\min_{w,b}L(w,b)$$

### **解析解**
线性回归刚好是一个很简单的优化问题。线性回归的解可以用一个公式简单地表达出来，首先，我们将偏置$b$合并到参数$w$中，合并方法是在包含所有参数的矩阵中附加一列。我们的预测问题是最小化$\lVert y-Xw \rVert^{2}$。将损失关于$w$的导数设为0，则得到解析解：
$$w^{*}=(X^{T}X)^{-1}y$$

### **随机梯度下降**
梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。

在每次迭代中，我们首先随机抽样一个小批量$B$，它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。最后，我们将梯度乘以一个预先确定的正数$\eta$作为学习率，并从当前参数的值中减掉：
$$(\mathbf{w}, b) \leftarrow(\mathbf{w}, b)-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w}, b)} l^{(i)}(\mathbf{w}, b)$$

``` python
def sgd(params, lr, batch_size):  #@save
    """小批量随机梯度下降"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

或者是

``` python
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
```

算法的步骤如下：
1. 初始化模型参数的值，如随机初始化
2. 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤

对于平方损失和仿射变换，我们可以明确地写成如下形式：
$$\mathbf{w} \leftarrow \mathbf{w}-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b)=\mathbf{w}-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right)$$

$$b \leftarrow b-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{b} l^{(i)}(\mathbf{w}, b)=b-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right)$$

### **正态分布与MSE**
正态分布和线性回归之间的关系很密切。简单的说，若随机变量$x$具有均值$\mu$和方差$\sigma$（标准差），其正态分布概率密度函数如下：
$$p(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right)$$

<div align=center><img src=../机器学习入门/深度学习/gaussion.svg></div>

就像我们所看到的，改变均值会产生沿轴$x$的偏移，增加方差将会分散分布、降低其峰值。均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。 噪声正态分布如下式:
$$y=w^{T}x+b+\epsilon$$
其中，$\epsilon \in \mathcal{N}(0,\sigma^{2})$。<br/>
因此，我们现在可以写出通过给定的观测到特定的似然（likelihood）：
$$P(y \mid \mathbf{x})=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}\left(y-\mathbf{w}^{\top} \mathbf{x}-b\right)^{2}\right)$$
现在，根据极大似然估计法，参数$w$和$b$的最优值是使整个数据集的似然最大的值：
$$P(\mathbf{y} \mid \mathbf{X})=\prod_{i=1}^{n} p\left(y^{(i)} \mid \mathbf{x}^{(i)}\right)$$
我们可以改为最小化负对数似然：
$$-\log P(\mathbf{y} \mid \mathbf{X})=\sum_{i=1}^{n} \frac{1}{2} \log \left(2 \pi \sigma^{2}\right)+\frac{1}{2 \sigma^{2}}\left(y^{(i)}-\mathbf{w}^{\top} \mathbf{x}^{(i)}-b\right)^{2}$$
现在我们只需要假设$\sigma$是某个固定常数就可以忽略第一项，因为第一项不依赖于$w$和$b$。现在第二项除了常数$\frac{1}{\sigma^{2}}$外，其余部分和前面介绍的均方误差是一样的。幸运的是，上面式子的解并不依赖于$\sigma$。因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。

## 2.softmax回归
### **分类问题**
假设每次输入是一个的灰度图像。 我们可以用一个标量表示每个像素值，每个图像对应四个特征$x_{1},x_{2},x_{3},x_{4}$。 此外，假设每个图像属于类别“猫”，“鸡”和“狗”中的一个。<br/><br/>
我们利用独热编码将类别进行编码$y\in \{(1,0,0),(0,1,0),(0,0,1)\}$，则这个分类问题就转换为了回归问题。

### **网络结构**
与线性回归一样，softmax回归也是一个单层神经网络。 由于计算每个输出$o_{1}$、$o_{2}$和$o_{3}$取决于所有输入$x_{1}$、$x_{2}$、$x_{3}$和$x_{4}$，所以softmax回归的输出层也是全连接层。<>

<div align=center><img src=../机器学习入门/深度学习/softmaxreg.svg></div>

### **全连接层开销**
正如我们将在后续章节中看到的，在深度学习中，全连接层无处不在。 然而，顾名思义，全连接层是“完全”连接的，可能有很多可学习的参数。具体来说，对于任何具有个$d$输入和$q$个输出的全连接层，参数开销为$\mathcal{O}(dq)$，这个数字在实践中可能高得令人望而却步。 幸运的是，将个输入转换为个输出的成本可以减少到$\mathcal{O}(\frac{dq}{n})$， 其中超参数$n$可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性 [Zhang et al., 2021](./reference.md)。

### **softmax运算**
softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。
$$\hat{y}=softmax(o),其中\hat{y_{i}}=\frac{exp(o_{j})}{\sum_{k}exp(o_{k})}$$

``` python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # 这里应用了广播机制
```

或者是

``` python
softmax = nn.Softmax(dim=0)
```

尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回归是一个线性模型（linear model）。<br/><br/>
同时可以使用Cross-Entropy,$H(y,\hat{y})=\sum_{i}-y_{i}log(\hat{y_{i}})$使得训练过程更加注重正确的样本。<br/><br/>
利用softmax的定义和交叉熵，我们得到：
$$\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=-\sum_{j=1}^{q} y_{j} \log \frac{\exp \left(o_{j}\right)}{\sum_{k=1}^{q} \exp \left(o_{k}\right)} \\
&=\sum_{j=1}^{q} y_{j} \log \sum_{k=1}^{q} \exp \left(o_{k}\right)-\sum_{j=1}^{q} y_{j} o_{j} \\
&=\log \sum_{k=1}^{q} \exp \left(o_{k}\right)-\sum_{j=1}^{q} y_{j} o_{j} .
\end{aligned}$$
考虑相对于任何未规范化的预测的导数$o_{j}$，我们得到：
$$\partial_{o_{j}} l(\mathbf{y}, \hat{\mathbf{y}})=\frac{\exp \left(o_{j}\right)}{\sum_{k=1}^{q} \exp \left(o_{k}\right)}-y_{j}=\operatorname{softmax}(\mathbf{o})_{j}-y_{j}$$
换句话说，导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 从这个意义上讲，这与我们在回归中看到的非常相似，其中梯度是观测值$y$和估计值$\hat{y}$之间的差异。

# 多层感知机
## 1.隐藏层
### **线性模型的问题**
线性意味着单调假设：任何特征的增大都会导致模型输出的增大（如果对应的权重为正），或者导致模型输出的减小（如果对应的权重为负）。然而我们可以很容易找出违反单调性的例子。例如，我们想要根据体温预测死亡率。对于体温高于37摄氏度的人来说，温度越高风险越大。然而，对于体温低于37摄氏度的人来说，温度越高风险就越低。

### 在网络中加入隐藏层
我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。

``` python
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    return (H@W2 + b2)
```

或者是

``` python
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))
```

<div align=center><img src=../机器学习入门/深度学习/mlp.svg></div>

### **从线性到非线性**
由于线性函数的线性函数还是线性函数，但是由于我们一层全连接层即可拟合线性模型，所以单纯的累加全连接层毫无意义。

为了发挥多层架构的潜力， 我们还需要一个额外的关键要素： 在仿射变换之后对每个隐藏单元应用非线性的激活函数（activation function）$\sigma$。激活函数的输出（例如，$\sigma()$）被称为活性值（activations）。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：
$$\begin{array}{l}
\mathbf{H}=\sigma\left(\mathbf{X} \mathbf{W}^{(1)}+\mathbf{b}^{(1)}\right) \\
\mathbf{O}=\mathbf{H} \mathbf{W}^{(2)}+\mathbf{b}^{(2)}
\end{array}$$

### **通用近似原理**
多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的。

## 2.激活函数
### **ReLU**
给定元素，ReLU函数被定义为该元素与的最大值：
$$ReLU(x)=max(0,x)$$

``` python
y = torch.relu(x)
```

<div align=center><img src=../机器学习入门/深度学习/relu.svg></div>

注意，ReLU函数有许多变体，包括参数化ReLU（Parameterized ReLU，pReLU） 函数 [He et al., 2015](./reference.md)。。 该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：
$$pReLU(x)=max(0,x)+\alpha min(0,x)$$

### **sigmoid**
sigmoid函数将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为挤压函数（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：
$$sigmoid(x)=\frac{1}{1+exp(-x)}$$
sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。然而，sigmoid在隐藏层中已经较少使用， 它在大部分时候被更简单、更容易训练的ReLU所取代。

``` python
y = torch.sigmoid(x)
```

<div align=center><img src=../机器学习入门/深度学习/sigmoid.svg></div>

### tanh
与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下：
$$tanh(x)=\frac{1-exp(-2x)}{1+exp(-2x)}$$

``` python
y = torch.tanh(x)
```

<div align=center><img src=../机器学习入门/深度学习/tanh.svg></div>

## 3.过/欠拟合
### **验证集**
我们不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。解决此问题的常见做法是将我们的数据分成三份，除了训练和测试数据集之外，还增加一个验证数据集（validation dataset），也叫验证集（validation set）。 
- K折交叉验证：当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是采用K折交叉验证。这里，原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在K-1个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对K次实验的结果取平均来估计训练和验证误差。

### **过拟合 $or$ 欠拟合**
- 模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足）。同时，训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为欠拟合（underfitting）。
- 当我们的训练误差明显低于验证误差时要小心， 这表明严重的过拟合（overfitting）。过拟合并不总是一件坏事。 特别是在深度学习领域，众所周知， 最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。

## 权重衰减
我们总是可以通过去收集更多的训练数据来缓解过拟合。 但这可能成本很高，耗时颇多，或者完全超出我们的控制，因而在短期内不可能做到。 假设我们已经拥有尽可能多的高质量数据，我们便可以将重点放在正则化技术上。

在多项式回归中，我们可以通过调整拟合多项式的阶数来限制模型的容量。实际上，限制特征的数量是缓解过拟合的一种常用技术。 然而，简单地丢弃特征对于这项工作来说可能过于生硬。比如对于多项式而言，给定$k$个变量，阶数为$d$的项的个数为 
$C^{k-1}_{k-1+d}=\frac{(k-1+d)!}{d!(k-1)!}$。 因此即使是阶数上的微小变化，也会显著增加我们模型的复杂性。

在训练参数化机器学习模型时， 权重衰减（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_{2}$正则化。

要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 将原来的训练目标最小化训练标签上的预测损失， 调整为最小化预测损失和惩罚项之和：
$$L(w,b)+\frac{\lambda}{2}\lVert w-0 \rVert^{2}$$

此外，你可能会问为什么我们首先使用$L_{2}$范数，而不是$L_{1}$范数。使用$L_{2}$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。在实践中，这可能使它们对单个变量中的观测误差更为稳定。

相较于原本的线性模型的随机梯度下降过程，正则化后的更新过程变为：
$$\mathbf{w} \leftarrow(1-\eta \lambda) \mathbf{w}-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right)$$

效果：
- 未使用正则化，训练数据loss迅速下降而测试数据loss几乎不变，显然出现了过拟合。

<div align=center><img src=../机器学习入门/深度学习/withoutL2.svg></div>

- 使用了$L_{2}$正则，过拟合现象有一定好转。

<div align=center><img src=../机器学习入门/深度学习/withL2.svg></div>

## 4.Dropout
### **重新审视过拟合**
当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。然而，线性模型无法考虑特征之间的关系，仅能简单的给某一特征标记为正或负权重。

泛化性和灵活性之间的这种基本权衡被描述为偏差-方差权衡（bias-variance tradeoff）。线性模型有很高的偏差：它们只能表示一小类函数。然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果。

深度神经网络位于偏差-方差谱的另一端。 与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。 例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件， 但单独出现则不表示垃圾邮件。

### **对扰动的稳健性**
什么是一个“好”的预测模型？ 我们期待“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。

一般我们肯定可以认为越小的模型就越简单，正如我们上面所讨论的那样，利用$L_{2}$范数度量简单性。

简单性的另一个角度是平滑性，即函数不应该对其输入的微小变化敏感。 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。

1995年，克里斯托弗·毕晓普证明了 具有输入噪声的训练等价于Tikhonov正则化 [Bishop, 1995](./reference.md)。 这项工作用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。然后在2014年，斯里瓦斯塔瓦等人 [Srivastava et al., 2014] 就如何将毕晓普的想法应用于网络的内部层提出了一个想法： 在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。

这个想法被称为暂退法（dropout）。 暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。

那么关键的挑战就是如何注入这种噪声。 一种想法是以一种无偏向（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。在毕晓普的工作中，他将高斯噪声添加到线性模型的输入中。在每次训练迭代中，他将从均值为零的分布$\epsilon \in \mathcal{N}(0,\sigma^{2})$采样噪声添加到输入$x$，从而产生扰动点$x^{\prime}x+\epsilon$，数学期望是$E[x^{\prime}]=x$

在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值以暂退概率由随机变量替换，如下所示：
$$h^{\prime}=\left\{\begin{array}{ll}
0 & \text { 概率为 } p \\
\frac{h}{1-p} & \text { 其他情况 }
\end{array}\right.$$
根据此模型的设计，其期望值保持不变，即$E[h^{\prime}]=h$

### **实践中的Dropout**
当我们将暂退法应用到隐藏层，以的概率将隐藏单元置为零时，结果可以看作是一个只包含原始神经元子集的网络。比如，删除了和$h_{2}和h_{5}$，因此输出的计算不再依赖于或，并且它们各自的梯度在执行反向传播时也会消失。

<div align=center><img src=../机器学习入门/深度学习/dropout.svg></div>

如上面公式所述，若以$p$的概率丢弃张量输入$X$中的元素， 如上所述重新缩放剩余部分：将剩余部分除以$1.0-p$。

``` python
def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃
    if dropout == 1:
        return torch.zeros_like(X)
    # 在本情况中，所有元素都被保留
    if dropout == 0:
        return X
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)
```

## 5.前向传播、反向传播和计算图
我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过前向传播（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。

### **前向传播**
前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。

为了简单起见，我们假设输入样本是$x\in \mathbb{R}^{d}$，并且我们的隐藏层不包括偏置项。这里的中间变量是：
$$z=W^{(1)}x$$
其中$W^{(1)}\in \mathbb{R}^{h\timesd}$是隐藏层的权重参数。将中间变量$z\in \mathbb{R}^{h}$通过激活函数$\phi$后，我们得到长度为$h$的隐藏激活向量：
$$h=\phi(z)$$
隐藏变量$h$也是一个中间变量。假设输出层的参数只有权重$W^{(2)}\in \mathbb{R}^{q\times dh}$，我们可以得到输出层变量，它是一个长度为$q$的向量：
$$o=W^{(2)}h$$
假设损失函数为$l$，样本标签为$y$，我们可以计算单个数据样本的损失项：
$$L=l(o,y)$$
根据正则化的定义，给定超参数$\lambda$，正则化项为：
$$s=\frac{\lambda}{2}(\lVert W^{(1)}) \rVert ^{2}_{F} \lVert W^{(2)}) \rVert ^{2}_{F}$$
其中矩阵的Frobenius范数是将矩阵展平为向量后应用的$L_{2}$范数。最后，模型在给定数据样本上的正则化损失为：
$$J=L+s$$
在下面的讨论中，我们将称$J$为目标函数（objective function）。

<div align=center><img src=../机器学习入门/深度学习/forward.svg></div>

### **反向传播**
反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。 

## 6.数值稳定性和模型初始化

### **梯度消失和梯度爆炸**
考虑一个具有$L$层、输入$x$和输$o$的深层网络。 每一层$l$由变换$f_{l}$定义， 该变换的参数为权重$W^{(l)}$，其隐藏变量是$h^{(l)}$（令$h^{(0)}=x$）。我们的网络可以表示为：
$$o=f_{l}\circ ... \circ f_{1}(x)$$
如果所有隐藏变量和输入都是向量，我们可以将$o$关于任何一组参数$W^{(l)}$的梯度写为下式：
$$\partial_{w^{(l)}}o=\partial_{h^{(L-1)}}h^{L} ... \partial_{h^{(l)}}h^{l+1} \partial_{w^{(l)}}h^{l}$$
换言之，该梯度是$L-1$个矩阵与梯度向量 的乘积。 因此，我们容易受到数值下溢问题的影响. 当将太多的概率乘在一起时，这些问题经常会出现。最初，矩阵$M^{(l)}$可能被初始化成各种各样的特征值。他们可能很小，也可能很大；他们的乘积可能非常大，也可能非常小。

我们可能面临一些问题。 要么是梯度爆炸（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛； 要么是梯度消失（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。

#### *梯度消失*
曾经sigmoid函数很流行，因为它类似于阈值函数。由于早期的人工神经网络受到生物神经网络的启发，神经元要么完全激活要么完全不激活（就像生物神经元）的想法很有吸引力。然而，它却是导致梯度消失问题的一个常见的原因，让我们仔细看看sigmoid函数为什么会导致梯度消失。

``` python
%matplotlib inline
import torch
from d2l import torch as d2l

x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.sigmoid(x)
y.backward(torch.ones_like(x))

d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],
        legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
 ```

<div align=center><img src=../机器学习入门/深度学习/grad_van.svg></div>

当sigmoid函数的输入很大或是很小时，它的梯度都会消失。 此外，当反向传播通过许多层时，除非我们在刚刚好的地方， 这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。 当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。

因此，更稳定的ReLU系列函数已经成为从业者的默认选择（虽然在神经科学的角度看起来不太合理）。

#### *梯度爆炸*
相反，梯度爆炸可能同样令人烦恼。为了更好地说明这一点，我们生成100个高斯随机矩阵，并将它们与某个初始矩阵相乘。对于我们选择的尺度（方差$\sigma^{2}=1$），矩阵乘积发生爆炸。当这种情况是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。

```python
M = torch.normal(0, 1, size=(4,4))
print('一个矩阵 \n',M)
for i in range(100):
    M = torch.mm(M,torch.normal(0, 1, size=(4, 4)))

print('乘以100个矩阵后\n', M)
```

```python
一个矩阵
 tensor([[ 0.4382, -0.7687,  0.2731, -0.2587],
        [-0.1789, -0.2395,  1.4915,  0.2634],
        [-0.5272,  0.2403,  2.4397, -0.7587],
        [ 0.9805,  0.4166, -0.1906, -0.2581]])
乘以100个矩阵后
 tensor([[ 7.6616e+22,  4.2587e+22, -5.8065e+22,  1.2980e+23],
        [-2.3790e+21, -1.3224e+21,  1.8030e+21, -4.0304e+21],
        [-1.3796e+23, -7.6687e+22,  1.0456e+23, -2.3373e+23],
        [ 8.5987e+20,  4.7795e+20, -6.5167e+20,  1.4567e+21]])
```

### **参数初始化**
解决（或至少减轻）上述问题的一种方法是进行参数初始化， 优化期间的注意和适当的正则化也可以进一步提高稳定性。

- 默认初始化<br/>
  如果我们不指定初始化方法， 框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。
- Xavier初始化<br/>
  通常，Xavier初始化从均值为零，方差$\sigma^{2}=\frac{2}{n_{in}+n_{out}}$的高斯分布中采样权重。我们也可以利用Xavier的直觉来选择从均匀分布中抽取权重时的方差。注意均匀分布$U\in(-a,a)$的方差为$\frac{a^{2}}{3}$。将$\frac{a^{2}}{3}$代入到$\sigma^{2}$的条件中，将得到初始化值域：
  $$U\left(-\sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}}, \sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}}\right)$$
  尽管在上述数学推理中，“不存在非线性”的假设在神经网络中很容易被违反， 但Xavier初始化方法在实践中被证明是有效的。

# 深度学习计算
## 1.层和块
为了实现这些复杂的网络，我们引入了神经网络块的概念。块（block）可以描述单个层、由多个层组成的组件或整个模型本身。使用块进行抽象的一个好处是可以将一些块组合成更大的组件，这一过程通常是递归的。

<div align=center><img src=../机器学习入门/深度学习/blocks.svg></div>

从编程的角度来看，块由类（class）表示。它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，并且必须存储任何必需的参数。注意，有些块不需要任何参数。最后，为了计算梯度，块必须具有反向传播函数。在定义我们自己的块时，由于自动微分提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。

在构造自定义块之前，我们先回顾一下多层感知机的代码。包含一个具有256个单元和ReLU激活函数的全连接隐藏层，然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。
``` python
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

X = torch.rand(2, 20)
net(X)
```

在这个例子中，我们通过实例化nn.Sequential来构建我们的模型，层的执行顺序是作为参数传递的。 简而言之，nn.Sequential定义了一种特殊的Module，即在PyTorch中表示一个块的类，它维护了一个由Module组成的有序列表。

### **自定义块**
在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能：
1. 将输入数据作为其前向传播函数的参数。
2. 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。
3. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。
4. 存储和访问前向传播计算所需的参数。
5. 根据需要初始化模型参数。

在下面的代码片段中，我们从零开始编写一个块。 它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。注意，下面的MLP类继承了表示块的Module类。我们的实现只需要提供我们自己的构造函数（Python中的__init__函数）和前向传播函数。

``` python
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))
```

注意：
1. 我们定制的__init__函数通过super().__init__() 调用父类的__init__函数， 省去了重复编写模版代码的痛苦。
2. 注意，除非我们实现一个新的运算符，否则我们不必担心反向传播函数或参数初始化，系统将自动生成这些。

### **顺序快**
现在我们可以更仔细地看看Sequential类是如何工作的， 回想一下Sequential的设计是为了把其他模块串起来。为了构建我们自己的简化MySequential， 我们只需要定义两个关键函数：
1. 一种将块逐个追加到列表中的函数。
2. 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。

下面的MySequential类提供了与默认Sequential类相同的功能。

``` python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X
```

__init__函数将每个模块逐个添加到有序字典_modules中。你可能会好奇为什么每个Module都有一个_modules属性？ 以及为什么我们使用它而不是自己定义一个Python列表？简而言之，_modules的主要优点是：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块。

当MySequential的前向传播函数被调用时，每个添加的块都按照它们被添加的顺序执行。现在可以使用我们的MySequential类重新实现多层感知机。

``` python
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```

### **在前向传播函数中执行代码**
当需要更强的灵活性时，我们需要定义自己的块。 例如，我们可能希望在前向传播函数中执行Python的控制流。 此外，我们可能希望执行任意的数学运算， 而不是简单地依赖预定义的神经网络层。

例如，我们需要一个计算函数$f(w,x)=cw^{T}x$的层，其中$x$是输入，$w$是参数，$c$是某个在优化过程中没有更新的指定常量。因此我们实现了一个FixedHiddenMLP类，如下所示：

``` python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```

## 2.参数管理
经过训练后，我们将需要使用这些参数来做出未来的预测。此外，有时我们希望提取参数，以便在其他环境中复用它们，将模型保存下来。

举个例子，我们有一个简单的MLP：

``` python
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)
```

当通过Sequential类定义模型时， 我们可以通过索引来访问模型的任意层。这就像模型是一个列表一样，每层的参数都在其属性中。如下所示，我们可以检查第二个全连接层的参数。

``` python
print(net[2].state_dict())
```